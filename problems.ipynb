{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d83dea9",
   "metadata": {},
   "source": [
    "# Computer Infrastructure\n",
    "\n",
    "This document will contain solutions/workings for the Computer Infrastructure module, the tasks completed in this are from the [problems.md](https://github.com/ianmcloughlin/computer-infrastructure/blob/main/assessment/problems.md) document for this module.\n",
    "The assessment details can be found in [assessment.md](https://github.com/ianmcloughlin/computer-infrastructure/blob/main/assessment/assessment.md) and in the [guidance notebook](https://github.com/ianmcloughlin/computer-infrastructure/blob/main/assessment/guidance.ipynb).\n",
    "\n",
    "Splitting this notebook into a number of different sections so that the problems can be completed consecutively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16704197",
   "metadata": {},
   "source": [
    "## Problem 1: Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38596b9",
   "metadata": {},
   "source": [
    "For this project the [yfinance](https://github.com/ranaroussi/yfinance/blob/main/README.md) package will be used. I used [https://www.youtube.com/watch?v=SxIwqdedomg](https://www.youtube.com/watch?v=SxIwqdedomg) for installing yfinance.\n",
    "\n",
    "From this, all hourly data for the previous 5 days for the 5 FAANG stocks:\n",
    "\n",
    "- Facebook (META)\n",
    "- Apple (AAPL)\n",
    "- Amazon (AMZN)\n",
    "- Netflix (NFLX)\n",
    "- Google (GOOG)\n",
    "\n",
    " File names will use the following format: YYYYMMDD-HHmmss.csv where YYYYMMDD is the four-digit year (e.g. 2025), followed by the two-digit month (e.g. 09 for September, 11 for November etc.), followed by the two digit day, and HHmmss is hour, minutes, seconds. To ensure correct file naming [https://forum.enterprisedna.co/t/adding-a-datetime-to-the-name-of-the-file/45592/4](https://forum.enterprisedna.co/t/adding-a-datetime-to-the-name-of-the-file/45592/4) and [https://community.jmp.com/t5/Discussions/Format-Date-Time-yyyymmdd-hhmmss/td-p/694172](https://community.jmp.com/t5/Discussions/Format-Date-Time-yyyymmdd-hhmmss/td-p/694172) were used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6058625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First need to make sure the correct modules are installed.\n",
    "# I decided to add all of these libraries because they are commonly used for data analysis and visualization and are allowed according to the requirements.\n",
    "# See: https://github.com/ianmcloughlin/computer-infrastructure/blob/main/requirements.txt\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "import statsmodels as sm\n",
    "import scipy as spy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5232cddb",
   "metadata": {},
   "source": [
    "It was important that while importing the data, needed to make sure to name it as appropriate in order to save it in an easily understandable and readable manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78e93ff",
   "metadata": {},
   "source": [
    "Need to first find out how to download the data for the FAANG stocks. All the hourly data for the past 5 days is required so this will be part of the process. See the [StackOverflow page](https://stackoverflow.com/questions/74479906/how-to-get-aggregate-4hour-bars-historical-stock-data-using-yfinance-in-python) for an example of collecting data over the last 2 years hourly - this will be adapted to be for the last 5 days and hourly. When importing the data and make sure that it's labelled appropriately so that each of the 5 FAANG can be differentiated from one another. \n",
    "\n",
    "```\n",
    "faang5_data = yf.download(tickers=\"AAPL AMZN META GOOG NFLX\", period=\"5d\", interval=\"1h\") \n",
    "```\n",
    "Where:\n",
    "- `tickers=\"AAPL AMZN META GOOG NFLX\"` is the company(ies) where the stock data is to be saved for.\n",
    "- `period=\"5d\"` is the period for which the data should be saved for.\n",
    "- and `interval=\"1h\"` is the intervals at which the data for each of the tickers should be included in the dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2506325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code downloads the hourly data for the last 5 days for the 5 FAANG stocks (Facebook, Apple, Amazon, Netflix, Google) using the yfinance package.\n",
    "\n",
    "faang5_data = yf.download(tickers=\"AAPL AMZN META GOOG NFLX\", period=\"5d\", interval=\"1h\") # adapted from https://stackoverflow.com/questions/74479906/how-to-get-aggregate-4hour-bars-historical-stock-data-using-yfinance-in-python\n",
    "\n",
    "def get_data(faang5_data): \n",
    "    print(faang5_data) # to check it works\n",
    "    return faang5_data\n",
    "\n",
    "# Now need to call the function to get the data\n",
    "get_data(faang5_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b919eec",
   "metadata": {},
   "source": [
    "I wanted to make sure that the files are saving to the correct location. I accidentally saved the data into a new folder that was in the higher file on my PC so I decided that an absolute path was the best way to go to minimise problems (I'll to the same with the plots later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4125ffa8",
   "metadata": {},
   "source": [
    "Originally the code written was:\n",
    "\n",
    "```# I kept getting an error that the folder didn't exist so I created it manually in the root repository.\n",
    "# So now I need to check that the directory exists before trying to save the file.\n",
    "import os\n",
    "output_dir = r\"D:\\Data_Analytics\\Modules\\CI\\computer-infrastructure\\data-faang-stocks\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    # Get the absolute path of the output directory\n",
    "absolute_output_dir = os.path.abspath(output_dir)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b56efe",
   "metadata": {},
   "source": [
    "Correction. \n",
    "\n",
    "Following Automation the above code has been modified so that the entire directory line is no longer inputted, only the file location for the save is input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f36e1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "output_dir = r\"data-faang-stocks/\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "    \n",
    "    # Get the absolute path of the output directory\n",
    "absolute_output_dir = os.path.abspath(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d5ca4e",
   "metadata": {},
   "source": [
    "Save data as csv in correct format name. I wanted to be sure that every file would be saved correctly in the `YYYYMMDD-HHmmss.csv` format. In the name `YYYYMMDD` gives the year (e.g. 2025) followed by the month (10 for October), followed by the date (e.g. 02 for 2nd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cb8711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to save it as a .csv file in a folder called data-faang-stocks in the root repository with the correct naming format YYYYMMDD-HHmmss.csv.\n",
    "# Save the data to a CSV file\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_path = os.path.join(absolute_output_dir, f\"{current_time}.csv\")\n",
    "faang5_data.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32fd63b",
   "metadata": {},
   "source": [
    "Verify save file:\n",
    "\n",
    "- The location it saved to.\n",
    "- The absolute path to the file directory.\n",
    "- A list of the files within the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89fe41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file was saved\n",
    "print(f\"File saved at: {file_path}\")\n",
    "print(f\"Absolute path to output directory: {absolute_output_dir}\")\n",
    "print(\"Files in directory:\", os.listdir(output_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ce08ed",
   "metadata": {},
   "source": [
    "Although the above code saved the file correctly, it did not however keep the time and date of each of the bits of data. I modified the code modifying it by removing the `index=False` argument, and replacing it with `date_format='%Y-%m-%d %H:%M:%S'`. The method for doing this was adapted  from [StackOverflow page](https://stackoverflow.com/questions/30298144/datetime-format-change-when-save-to-csv-file-python)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55496f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now to save it as a .csv file in a folder called data-faang-stocks in the root repository with the correct naming format YYYYMMDD-HHmmss.csv. and make sure I don't lose the date and time index.\n",
    "# Save the data to a CSV file\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_path = os.path.join(absolute_output_dir, f\"{current_time}.csv\")\n",
    "faang5_data.to_csv(file_path, date_format='%Y-%m-%d %H:%M:%S') # adapted from https://stackoverflow.com/questions/30298144/datetime-format-change-when-save-to-csv-file-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60709ede",
   "metadata": {},
   "source": [
    "## Problem 2: Plotting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb719f1",
   "metadata": {},
   "source": [
    "For this part of the project I decided to use my work from the PANDS project from term 1 (2025) for saving images and exporting stuff to folders [HERE](https://github.com/KaiiMenai/pands-project), specifically in the [analysis.py](https://github.com/KaiiMenai/pands-project/blob/main/analysis.py) program."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f36ef7",
   "metadata": {},
   "source": [
    "Defining the function.\n",
    "Although this was rather long winded, I wanted to ensure that there were things in place should the data not be found, so that I could go back and fix the error. The first thing to do was to check that there were indeed csv files present in the specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178429e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data():\n",
    "    # Get the list of files in the directory\n",
    "    files = os.listdir(absolute_output_dir)\n",
    "    # Filter out only CSV files\n",
    "    csv_files = [f for f in files if f.lower().endswith('.csv')]\n",
    "    if not csv_files:\n",
    "        print(f\"No CSV files found in {absolute_output_dir}\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cab13a",
   "metadata": {},
   "source": [
    "Now that it has been determined that there are csv files present, they needed to be put in date-time order so that only the most recent file would be used for the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774b55f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Sort the files by modification time in descending order and pick latest\n",
    "    csv_files.sort(key=lambda x: os.path.getmtime(os.path.join(absolute_output_dir, x)), reverse=True)\n",
    "    latest_file = csv_files[0]\n",
    "    latest_file_path = os.path.join(absolute_output_dir, latest_file)\n",
    "    print(f\"Using latest CSV: {latest_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815bb178",
   "metadata": {},
   "source": [
    "The tickers (data sources) needed to be listed next.\n",
    "\n",
    "- AAPL - Apple\n",
    "- AMZN - Amazon\n",
    "- META - Meta (facebook, Instagram, WhatsApp)\n",
    "- GOOG - Google\n",
    "- NFLX - Netflix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d5956",
   "metadata": {},
   "outputs": [],
   "source": [
    "    tickers = [\"AAPL\", \"AMZN\", \"META\", \"GOOG\", \"NFLX\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bdec79",
   "metadata": {},
   "source": [
    "After a few tries and manual checks, I realised that the csv file was saving with multi-line headers. The file was to be saved with a 3-line header [Price / Ticker / Datetime](https://sparkbyexamples.com/pandas/pandas-multiindex-dataframe-examples/#:~:text=Pandas%20MultiIndex%20to%20Columns,to%20DataFrame%20starting%20from%20zero).\n",
    "To mitigate this as an issue, I needed to ensure that some of the lines in the csv were skipped ([1](https://towardsdatascience.com/working-with-multi-index-pandas-dataframes-f64d2e2c3e02/), [2](https://pandas.pydata.org/docs/user_guide/advanced.html), and [3](https://www.datacamp.com/tutorial/pandas-read-csv))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3da8760",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Try reading the CSV that was saved with a 3-line header (Price / Ticker / Datetime) https://sparkbyexamples.com/pandas/pandas-multiindex-dataframe-examples/#:~:text=Pandas%20MultiIndex%20to%20Columns,to%20DataFrame%20starting%20from%20zero.\n",
    "    try:\n",
    "        # The CSV has a multi-row header: first row contains Price/Close/High/..., second the tickers. - https://towardsdatascience.com/working-with-multi-index-pandas-dataframes-f64d2e2c3e02/, https://pandas.pydata.org/docs/user_guide/advanced.html\n",
    "        # The third row in the file contains the literal 'Datetime' in the first column which will be skipped.\n",
    "        data = pd.read_csv(latest_file_path, header=[0, 1], skiprows=[2], index_col=0, parse_dates=True)\n",
    "    except Exception as e:\n",
    "        print(\"Warning: failed to read CSV with MultiIndex header, trying fallback. Error:\", e) # This will help find error cause.\n",
    "        # Try to read normally and attempt to promote the first data row to column level 1 if it contains tickers - not entirely necessary but noted just incase\n",
    "        data = pd.read_csv(latest_file_path, index_col=0, parse_dates=True)\n",
    "        # If the first row contains ticker symbols, promote it to second-level header -  https://www.datacamp.com/tutorial/pandas-read-csv\n",
    "        try:\n",
    "            first_row = data.iloc[0]\n",
    "            # if the first row contains one of the expected tickers, use it as second header \n",
    "            if any(str(x) in tickers for x in first_row.values):\n",
    "                new_cols = pd.MultiIndex.from_arrays([data.columns, first_row.values])\n",
    "                data = data[1:]\n",
    "                data.columns = new_cols\n",
    "                data.index = pd.to_datetime(data.index)\n",
    "                print(\"Promoted first data row to MultiIndex column level\")\n",
    "        except Exception:\n",
    "            # If anything goes wrong, continue available columns will be detected with the following\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dc531d",
   "metadata": {},
   "source": [
    "To ensure that the plot would only contain the essential data - closing prices for each of the companies (tickers) hourly, for the 5 days prior to the csv creation date, a [specific dataframe](https://realpython.com/pandas-dataframe/) `close_df` was created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68318ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Make a dataframe of closing prices called close_df. - https://realpython.com/pandas-dataframe/\n",
    "    close_df = None\n",
    "    if isinstance(data.columns, pd.MultiIndex): # Using the if/else will help avoid errors if the CSV doesn't have multi-level columns. (but I know it does after a manual check).\n",
    "        # Normally, it will be: top-level contains 'Close', second-level contains tickers\n",
    "        if 'Close' in data.columns.get_level_values(0):\n",
    "            close_df = data['Close']\n",
    "        else:\n",
    "            # Try to find a top-level name that includes 'Close'\n",
    "            top_levels = list(dict.fromkeys(data.columns.get_level_values(0)))\n",
    "            print(\"Top-level column names found:\", top_levels)\n",
    "            for name in top_levels:\n",
    "                if 'Close' in str(name):\n",
    "                    close_df = data[name]\n",
    "                    break\n",
    "    else:\n",
    "        # Single-level columns: try to find columns that mention 'Close'\n",
    "        close_cols = [c for c in data.columns if 'Close' in str(c)]\n",
    "        if close_cols:\n",
    "            close_df = data[close_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341a4320",
   "metadata": {},
   "source": [
    "Should the `close_df`not be created, then an error message will be raised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec58bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "    if close_df is None:\n",
    "        print(\"Could not find 'Close' columns. Available columns (first 30):\")\n",
    "        print(list(data.columns)[:30])\n",
    "        raise KeyError(\"Close columns not found in CSV - file format not recognised\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6312704e",
   "metadata": {},
   "source": [
    "Now for the plots themselves. Using the `close_df` previously created from the most recent saved csv, the closing share prices for each of the 5 tickers (companies) was created.\n",
    "The figure size was specified, values were checked to make sure that they were numeric (otherwise there would be an error in the plotting), and should there be any missing closing values for any of the tickers, then a message would be given `\"Ticker {ticker} not found in Close data; available: {list(close_df.columns)}\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58455332",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Plot the closing prices of each stock over time (only when available) - this will be done per hourly data over the 5 days of the data (included in the .csv).\n",
    "    fig, ax = plt.subplots(figsize=(14, 7))\n",
    "    for ticker in tickers:\n",
    "        if ticker in close_df.columns:\n",
    "            # Make sure values are numeric\n",
    "            series = pd.to_numeric(close_df[ticker], errors='coerce')\n",
    "            ax.plot(series.index, series.values, label=ticker)\n",
    "        else:\n",
    "            print(f\"Ticker {ticker} not found in Close data; available: {list(close_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f386ac26",
   "metadata": {},
   "source": [
    "Setting the details for the plot itself:\n",
    "\n",
    "- Plot title - \"FAANG Stock Closing Prices - {latest_file.split('.')[0]}\" - Latest file name will be in the date-time format.\n",
    "- x axis label - Date\n",
    "- y axis label - Closing Price (USD)\n",
    "- inclusion of a legend\n",
    "- putting a grid on the plot\n",
    "- making sure that the plots have a tight layout.\n",
    "\n",
    "Following the adding of the specifications to the plot, the plot was shown, and in order to make the plot easier for access for saving the custom function was then referred to as fig, `fig = plot_data()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6673b45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    ax.set_title(f\"FAANG Stock Closing Prices - {latest_file.split('.')[0]}\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Closing Price (USD)\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    fig.tight_layout()\n",
    "    # Show the plot interactively (optional) and return the Figure so it can be saved\n",
    "    plt.show()\n",
    "    return fig\n",
    "    \n",
    "# Now to call the function to plot the data and capture the Figure for saving\n",
    "fig = plot_data() # easier to access for saving."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05963325",
   "metadata": {},
   "source": [
    "For saving the plots I decided to keep the same format for saving as I did for the csv files. I did this by first specifying the save location. Again, following automation I realised the error I had initially made, I have only copied the corrected version into this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bbe6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_plot_dir = r\"plots-faang-stocks/\"\n",
    "if not os.path.exists(output_plot_dir):\n",
    "    os.makedirs(output_plot_dir)\n",
    "    # Get the absolute path of the output directory\n",
    "absolute_output_plot_dir = os.path.abspath(output_plot_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fecf32b",
   "metadata": {},
   "source": [
    "I also decided to use the current time as the file name. As this would run straight away following the saving of the csv data file, the date-time for the plot png would correspond to that of the csv. I used the same reference for saving the named in the datetime format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5fcf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the plot to a PNG file using the returned Figure - modified code from what was being used for csv saving\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "file_plot_path = os.path.join(absolute_output_plot_dir, f\"{current_time}.png\")\n",
    "if fig is not None:\n",
    "    try:\n",
    "        fig.savefig(file_plot_path, bbox_inches='tight', dpi=150)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save figure: {e}\")\n",
    "else:\n",
    "    print(\"No figure returned from plot_data(); nothing to save.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd3849",
   "metadata": {},
   "source": [
    "Finally, I wanted to check that the png file saved correctly. For this I made sure that the output would print out the file location, the directory path, and list all of the files in the save location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe70c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the file was saved\n",
    "print(f\"File saved at: {file_plot_path}\")\n",
    "print(f\"Absolute path to output directory: {absolute_output_plot_dir}\")\n",
    "print(\"Files in directory:\", os.listdir(output_plot_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a617f56",
   "metadata": {},
   "source": [
    "## Problem 3: Script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18efb4c6",
   "metadata": {},
   "source": [
    "FROM THE TASK OUTLINE\n",
    "\n",
    "What this problem involves:\n",
    "\n",
    "Create a Python script called faang.py in the root of your repository. Copy the above functions into it and it so that whenever someone at the terminal types ./faang.py, the script runs, downloading the data and creating the plot. Note that this will require a shebang line and the script to be marked executable. Explain the steps you took in your notebook.\n",
    "\n",
    "**AGAIN, refer to the PANDS project for the function for aid in the run and save parts of the script.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ede69e",
   "metadata": {},
   "source": [
    "## Problem 4: Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ace1b11",
   "metadata": {},
   "source": [
    "FROM THE TASK OUTLINE\n",
    "\n",
    "What this problem involves:\n",
    "\n",
    "Create a GitHub Actions workflow to run your script every Saturday morning. The script should be called faang.yml in a .github/workflows/ folder in the root of your repository. In your notebook, explain each of the individual lines in your workflow.\n",
    "\n",
    "References to help with this task:\n",
    "\n",
    "- [GitHub Actions workflow](https://docs.github.com/en/actions)\n",
    "- [CI/CD Tutorial using GitHub Actions - Automated Testing & Automated Deployments](https://youtu.be/YLtlz88zrLg) on YT\n",
    "- [trigger GitHub Actions on every Saturday](https://stackoverflow.com/questions/75974691/how-can-i-trigger-github-actions-on-every-3rd-saturday-of-a-month) on StackOverflow\n",
    "- [Run on a schedule](https://jasonet.co/posts/scheduled-actions)\n",
    "\n",
    "\n",
    "- STEP BU STEP - https://medium.com/@songulerdemguler/running-scheduled-code-on-github-8e6a03a8d88b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe10a40",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
